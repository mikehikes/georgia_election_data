{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction and Loading of Georgia Election Source Data\n",
    "\n",
    "The notebooks in this repo require a processed, cleaned, and normalized version of the source election data from the State of Georgia's Office of Secretary of State. This notebook, and an associated python script, will create a sqlite3-based database that subsequent notebooks will use as a data source.\n",
    "\n",
    "### Why sqlite3?\n",
    "This library is used to permit running the Jupyter notebooks on a laptop or Google Colab-type environment, without the need for purchasing or establishing a Google Cloud Compute (GCS) or AWS Resource. Future versions of this notebook may include the option to use paid-database resources on one or both of these services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites and Considerations\n",
    "\n",
    "1. Anaconda Python is used to create the environment needed to operate this notebook, with an environment defined in `environment.yml`. Anaconda Python may be [downloaded without cost](https://www.anaconda.com/distribution/#download-section) for Linux, Windows, or macOS.\n",
    "\n",
    "\n",
    "2. __This repo should be cloned from GitHub__; the notebook may not operate as expected if individual notebooks are downloaded without supporting files.\n",
    "\n",
    "\n",
    "3. This notebook was tested on a linux-based file-system. There may be unexpected behavior if operated on a Windows-based file-system.\n",
    "\n",
    "\n",
    "4. If you are running this notebook on a temporary instance (e.g., Google Colab), you may want to consider saving the results of this notebook in a persistent storage space.\n",
    "\n",
    "\n",
    "5. At least 4GB of free memory space is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of source data contained within the repo\n",
    "source_data_loc = 'source_data'\n",
    "\n",
    "# The current vintage of the data located in the repo\n",
    "data_vintage_loc = '20190709'\n",
    "\n",
    "# If not set here, the notebook will assume that you are running the notebook in the\n",
    "# root directory of the repo.\n",
    "working_directory = None\n",
    "#working_directory = '/home/michael/git_repos/georgia_election_data'\n",
    "\n",
    "# The first and last years' of data in the voter database\n",
    "first_years_data = 1996\n",
    "last_years_data = 2019\n",
    "\n",
    "# The sub-folder that will contain a sqlite3 database with the processed data\n",
    "database_location = 'processed_data'\n",
    "\n",
    "# The name of the file containing the election data\n",
    "dest_db_name = 'election_data.db'\n",
    "\n",
    "#The name of the table in the database containing the election results.\n",
    "dest_db_table_name = 'gaelect'\n",
    "\n",
    "# The batch size for processing.\n",
    "# IMPORTANT NOTE: If you are receiving out-of-memory errors, reduce the batch size.\n",
    "# Note that smaller batch size will result in slower performance.\n",
    "\n",
    "batch_size = int(3e6)\n",
    "\n",
    "# From the variables above, the full path to the sqlite3 database.\n",
    "full_path_to_db = f'{working_directory}/{database_location}/{dest_db_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Source Data\n",
    "\n",
    "Included in this repo are the source ZIP files obtained from the State of Georgia Secretary of State in August 2019. _These files are provided without assertions to accuracy_. To re-acquire or download the source files, you may follow the following instructions.\n",
    "\n",
    "#### 2013-2019\n",
    "Step 1: Go to https://elections.sos.ga.gov/Elections/voterhistory.do\n",
    "Step 2: Select the election year, and then download the `Full Year File`\n",
    "Step 3: Download each of the ZIP files into a folder accessible to your Python instance\n",
    "\n",
    "#### 1996-2012\n",
    "Step 1: Go to https://elections.sos.ga.gov/Elections/voterhistoryprevious.do\n",
    "Step 2: Download each year's zip file into a folder accessible to your Python instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming the existence and integrity of the source data\n",
    "\n",
    "The following cells indicate configuration parameters that describe the location and composition of the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory is /home/michael/git_repos/georgia_election_data/etl\n",
      "Found all expected data.\n"
     ]
    }
   ],
   "source": [
    "print(f'Current Working Directory is {os.getcwd()}')\n",
    "\n",
    "if working_directory is None:\n",
    "    working_directory = os.getcwd()\n",
    "\n",
    "if working_directory.split('/')[-1] != 'georgia_election_data':\n",
    "    print(\"Change the new_working_directory folder to indicate the root folder of this notebook\")\n",
    "        \n",
    "else:    \n",
    "    list_of_known_years_election_data = [str(a) + '.zip' for a in list(range(first_years_data,last_years_data+1))]\n",
    "\n",
    "found_files = sorted(os.listdir(working_directory + f'/{source_data_loc}' + f'/{data_vintage_loc}'))\n",
    "matches = found_files == list_of_known_years_election_data\n",
    "if matches:\n",
    "    print(\"Found all expected data.\")\n",
    "else:\n",
    "    print(f\"Expected data not found. Missing files are {list(set(list_of_known_years_election_data)-set(found_files))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voter Data  ETL Loading Functions\n",
    "\n",
    "These functions load the two available vintages of Georgia Voter Data: data for years 2012 and prior, and data for years 2013 and subsequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function_2012_prior(in_line):\n",
    "\n",
    "    in_line = in_line.decode('utf-8')\n",
    "    \n",
    "    county_no = in_line[0:3]\n",
    "    reg_no = in_line[3:11]\n",
    "    election_date = in_line[11:19]\n",
    "    election_type = in_line[19:22].strip()\n",
    "    party = in_line[22:23].strip()\n",
    "    absentee = in_line[23:24]\n",
    "    \n",
    "    # dates for this time series are presented as day-month-year\n",
    "    # we will standardize these dates to year-month-day to reduce\n",
    "    # confusion and match the date format of the 2013 and onwards data\n",
    "    #print(election_date)\n",
    "    election_date = election_date[4:] + '-' + election_date[0:2] + '-' + election_date[2:4]\n",
    "    \n",
    "    # standardize party indicator from primary election records: single char\n",
    "    # D -> Democrat\n",
    "    # R -> Republican\n",
    "    # N -> Non-Partisan\n",
    "    \n",
    "    if party == 'NP':\n",
    "        party = 'N'\n",
    "    elif party != 'D' and party != 'R' and party != 'N':\n",
    "        party = \"\"\n",
    "\n",
    "\n",
    "    if absentee == 'Y':\n",
    "        absentee = True\n",
    "    else:\n",
    "        absentee = False\n",
    "        \n",
    "    return (0,county_no, reg_no, election_date,election_type, party, absentee,None,None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function_2013_post(in_line):\n",
    "\n",
    "    # Convert all strings to utf-8 for international standarization purposes\n",
    "    in_line = in_line.decode('utf-8')\n",
    "    \n",
    "    county_no = in_line[0:3].strip()\n",
    "    reg_no = in_line[3:11].strip()\n",
    "    election_date = in_line[11:19].strip()\n",
    "    election_type = in_line[19:22].strip()\n",
    "    party= in_line[22:24].strip()\n",
    "    absentee =in_line[24:25].strip()\n",
    "    provisional = in_line[25:26].strip()\n",
    "    supplemental = in_line[26:27].strip()\n",
    "    \n",
    "    election_date = election_date[0:4] + '-' + election_date[4:6] + '-' + election_date[6:]    \n",
    "    # the election date is already in YYYY-MM-DD, so no additional action needed\n",
    "    \n",
    "    # standardize party indicator from primary election records: single char\n",
    "    # D -> Democrat\n",
    "    # R -> Republican\n",
    "    # N -> Non-Partisan\n",
    "    \n",
    "    if party == 'NP':\n",
    "        party = 'N'\n",
    "    elif party != 'D' and party != 'R' and party != 'N':\n",
    "        party = None\n",
    "\n",
    "    # Convert absentee flags to True/False\n",
    "\n",
    "    if absentee == 'Y':\n",
    "        absentee = True\n",
    "    else:\n",
    "        absentee = False\n",
    "\n",
    "    # Convert provisional flags to True/False\n",
    "\n",
    "    if provisional == 'Y':\n",
    "        provisional = True\n",
    "    else:\n",
    "        provisional = False        \n",
    "\n",
    "    # Convert supplemental flags to True/False\n",
    "    \n",
    "    if supplemental == 'Y':\n",
    "        supplemental = True\n",
    "    else:\n",
    "        supplemental = False       \n",
    "    return (1,county_no, reg_no, election_date,election_type, party, absentee,supplemental,absentee)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_elections_db_exists(full_path_to_db):\n",
    "    found = False\n",
    "    # first, check to see if the database file itself exists\n",
    "    if os.path.exists(full_path_to_db):\n",
    "        db = sqlite3.connect(full_path_to_db)\n",
    "        c = db.cursor()\n",
    "        c.execute(f'SELECT name FROM sqlite_master WHERE type=\\'table\\' AND name={dest_db_table_name}')\n",
    "    else:\n",
    "        return found\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_lines_iterator(source_file):\n",
    "    with ZipFile(source_file).open(ZipFile(source_file).namelist()[0]) as file:\n",
    "        for i in file:\n",
    "            yield i\n",
    "            \n",
    "#list_of_files = glob(working_directory + f'/{source_data_loc}' + f'/{data_vintage_loc}' + '/*')\n",
    "#source_file = list_of_files[1]\n",
    "#lines_required = batch_size\n",
    "\n",
    "# get number of lines in text file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_etl_batches(source_file):\n",
    "    with ZipFile(source_file).open(ZipFile(source_file).namelist()[0]) as f:\n",
    "        line_count = sum(1 for _ in f)\n",
    "        print(f'Datafile {ZipFile(source_file).namelist()[0]} contains {line_count} records')\n",
    "        in_range = list(range(line_count))\n",
    "        out_batch = [in_range[i * batch_size:(i + 1) * batch_size] for i in range((len(in_range) + batch_size - 1) // batch_size )] \n",
    "        return({'source_file':source_file,\n",
    "                'line_count':line_count,\n",
    "                'batches':[(min(b), max(b)) for b in out_batch]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 to process.\n",
      "Datafile Voter History 1996.txt contains 4644200 records\n",
      "Datafile Voter History 1997.txt contains 959435 records\n",
      "Datafile Voter History 1998.txt contains 3553259 records\n",
      "Datafile Voter History 1999.txt contains 539933 records\n",
      "Datafile Voter History 2000.txt contains 5046847 records\n",
      "Datafile Voter History 2001.txt contains 708042 records\n",
      "Datafile Voter History 2002.txt contains 3731165 records\n",
      "Datafile Voter History 2003.txt contains 602137 records\n",
      "Datafile Voter History 2004.txt contains 6399634 records\n",
      "Datafile Voter History 2005.txt contains 651522 records\n",
      "Datafile Voter History 2006.txt contains 3796362 records\n",
      "Datafile Voter History 2007.txt contains 752749 records\n",
      "Datafile Voter History 2008.txt contains 9628482 records\n",
      "Datafile Voter History 2009.txt contains 560700 records\n",
      "Datafile Voter History 2010.txt contains 4841793 records\n",
      "Datafile Voter History 2011.txt contains 877001 records\n",
      "Datafile Voter History 2012.txt contains 7036094 records\n",
      "Datafile 2013.TXT contains 630456 records\n",
      "Datafile 2014.TXT contains 4271520 records\n",
      "Datafile 2015.TXT contains 566225 records\n",
      "Datafile 2016.TXT contains 7727053 records\n",
      "Datafile 2017.TXT contains 1264641 records\n",
      "Datafile 2018.TXT contains 7458650 records\n",
      "Datafile 2019.TXT contains 205545 records\n"
     ]
    }
   ],
   "source": [
    "list_of_files = glob(working_directory + f'/{source_data_loc}' + f'/{data_vintage_loc}' + '/*')\n",
    "list_of_files.sort()\n",
    "print(f\"Found {len(list_of_files)} to process.\")\n",
    "ret_batches = list(map(create_etl_batches, list_of_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 76453445 total voting records and 42 batches in the data sets\n"
     ]
    }
   ],
   "source": [
    "number_of_records = sum([r['batches'][-1][1]+1 for r in ret_batches])\n",
    "number_of_batches = sum([len(r['batches']) for r in ret_batches])\n",
    "print(f'There are {number_of_records} total voting records and {number_of_batches} batches in the data sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fwf_to_sqlitedb(file_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes the source data files and builds a sqlite database.\n",
    "    CSVs are being used in lieu of pandas dataframes for the source data because\n",
    "    the memory footprint and CPU overhead of 78 million records requires an\n",
    "    un-needed quantity of those resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    db = sqlite3.connect(full_path_to_db)\n",
    "\n",
    "    # now load the files\n",
    "\n",
    "    pre_2012_range = set(range(1996,2013))\n",
    "    post_2013_range = set(range(2013,2020))\n",
    "\n",
    "    cur_file_name = file_dict['source_file']\n",
    "    \n",
    "    # collect the records\n",
    "    gen = get_n_lines_iterator(cur_file_name)\n",
    "        \n",
    "    for cur_range in tqdm(file_dict['batches'], position=0, desc=f'Processing batch in {os.path.basename(cur_file_name).split(\".\")[0]}'):\n",
    "        start_i, end_i = cur_range\n",
    "        end_i = end_i + 1\n",
    "        cur_record_list = [next(gen) for r in range(start_i, end_i)]\n",
    "        if int(os.path.basename(cur_file_name).split(\".\")[0]) in pre_2012_range:\n",
    "            parsed_data = list(map(parse_function_2012_prior,cur_record_list))\n",
    "\n",
    "        elif int(os.path.basename(cur_file_name).split(\".\")[0]) in post_2013_range:\n",
    "            parsed_data = list(map(parse_function_2013_post,cur_record_list))\n",
    "\n",
    "        #print(f\"Loading {os.path.basename(f)} into the sqlite database\")\n",
    "        c = db.cursor()\n",
    "        c.executemany(f\"INSERT INTO {dest_db_table_name} VALUES (?,?, ?, ?, ?, ?, ?, ?, ?)\", parsed_data)\n",
    "        db.commit()\n",
    "        #print(f\"Loaded {os.path.basename(f)} into the sqlite database\")\n",
    "\n",
    "    db.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function drops and creates a new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_create_database():\n",
    "\n",
    "    db = sqlite3.connect(full_path_to_db)\n",
    "    c = db.cursor()\n",
    "    c.execute(f'DROP TABLE IF EXISTS {dest_db_table_name}')\n",
    "    db.commit()\n",
    "              \n",
    "    c = db.cursor()\n",
    "    c.execute(f'''\n",
    "               CREATE TABLE {dest_db_table_name}(\n",
    "               vintage INT,\n",
    "               county_no TEXT,\n",
    "               reg_no TEXT, \n",
    "               election_date TEXT,\n",
    "               election_type TEXT, \n",
    "               party TEXT,\n",
    "               absentee BOOLEAN,\n",
    "               supplemental BOOLEAN,\n",
    "               provisional BOOLEAN) \n",
    "            ''')\n",
    "    db.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_and_create_database()\n",
    "\n",
    "# If you receive a database locking error, \n",
    "# it is likely that the database has been corrupted from an incomplete transaction.\n",
    "# You will likely need to delete the source database file to resolve and then run the function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch in 1996: 100%|██████████| 2/2 [00:24<00:00, 13.68s/it]\n",
      "Processing batch in 1997: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\n",
      "Processing batch in 1998: 100%|██████████| 2/2 [00:20<00:00, 13.05s/it]\n",
      "Processing batch in 1999: 100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n",
      "Processing batch in 2000: 100%|██████████| 2/2 [00:28<00:00, 15.01s/it]\n",
      "Processing batch in 2001: 100%|██████████| 1/1 [00:04<00:00,  4.04s/it]\n",
      "Processing batch in 2002: 100%|██████████| 2/2 [00:22<00:00, 14.11s/it]\n",
      "Processing batch in 2003: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it]\n",
      "Processing batch in 2004: 100%|██████████| 3/3 [00:42<00:00, 15.08s/it]\n",
      "Processing batch in 2005: 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]\n",
      "Processing batch in 2006: 100%|██████████| 2/2 [00:25<00:00, 16.02s/it]\n",
      "Processing batch in 2007: 100%|██████████| 1/1 [00:05<00:00,  5.15s/it]\n",
      "Processing batch in 2008: 100%|██████████| 4/4 [01:09<00:00, 16.98s/it]\n",
      "Processing batch in 2009: 100%|██████████| 1/1 [00:03<00:00,  3.32s/it]\n",
      "Processing batch in 2010: 100%|██████████| 2/2 [00:29<00:00, 16.12s/it]\n",
      "Processing batch in 2011: 100%|██████████| 1/1 [00:05<00:00,  5.01s/it]\n",
      "Processing batch in 2012: 100%|██████████| 3/3 [00:47<00:00, 15.82s/it]\n",
      "Processing batch in 2013: 100%|██████████| 1/1 [00:04<00:00,  4.97s/it]\n",
      "Processing batch in 2014: 100%|██████████| 2/2 [00:30<00:00, 17.75s/it]\n",
      "Processing batch in 2015: 100%|██████████| 1/1 [00:04<00:00,  4.39s/it]\n",
      "Processing batch in 2016: 100%|██████████| 3/3 [00:57<00:00, 19.41s/it]\n",
      "Processing batch in 2017: 100%|██████████| 1/1 [00:09<00:00,  9.03s/it]\n",
      "Processing batch in 2018: 100%|██████████| 3/3 [01:10<00:00, 23.80s/it]\n",
      "Processing batch in 2019: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "_ = list(map(convert_fwf_to_sqlitedb,ret_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for completeness\n",
    "\n",
    "The database should now contain as many records as there are rows in the source data. The following code will conduct an assertion test for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database has 76453445 rows, compared with 76453445 in the source data\n",
      "The Georgia Voter database appears to have been correctly loaded into the sqlite database.\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(full_path_to_db)\n",
    "c = db.cursor()\n",
    "c.execute(f'''SELECT count(*) FROM {dest_db_table_name}''')\n",
    "database_rows = c.fetchall()[0][0]\n",
    "db.close()\n",
    "\n",
    "print(f'The database has {database_rows} rows, compared with {number_of_records} in the source data')\n",
    "if database_rows == number_of_records:\n",
    "    print('The Georgia Voter database appears to have been correctly loaded into the sqlite database.')\n",
    "else:\n",
    "    print('Sorry, not all Georgia voter records were loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding helper tables\n",
    "\n",
    "There is one helper table that will also be added to the database. This helper table maps the election_type field to indicate if the field reflects a _primary_ election, a _primary runoff_ election, a _general_ election, or another type of vote.\n",
    "\n",
    "This mapping is needed because the election_type coding has either 1) not been consistent from election to election or 2) a new election_type code was created when a ballot combined two types of elections. For example, a general election added to a recall election.\n",
    "\n",
    "The repo includes a file `election_type_mapping.csv` that contains a manual classification between the election types and the above manual classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found helper data here:/home/michael/git_repos/georgia_election_data/auxillary_files/election_type_mapping.csv\n",
      "Table created\n",
      "Data uploaded\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(full_path_to_db)\n",
    "c = db.cursor()\n",
    "\n",
    "manual_classification_file_loc = working_directory + '/auxillary_files/election_type_mapping.csv'\n",
    "\n",
    "if os.path.exists(manual_classification_file_loc):\n",
    "    print(\"found helper data here:\" + manual_classification_file_loc)\n",
    "\n",
    "# load the file into a list of tuples\n",
    "\n",
    "input_str = open(manual_classification_file_loc, 'r').readlines()\n",
    "input_str_list = [tuple(l.strip().split(',')) for l in input_str]\n",
    "# remove header\n",
    "input_str_list = input_str_list[1:]\n",
    "\n",
    "c.execute('drop table if exists ga_elect_manual_classification;')\n",
    "db.commit()\n",
    "c = db.cursor()\n",
    "c.execute('''\n",
    "           CREATE TABLE IF NOT EXISTS ga_elect_manual_classification(\n",
    "           election_type_index INT,\n",
    "           election_type TEXT,\n",
    "           election_type_description TEXT, \n",
    "           manual_classification TEXT)\n",
    "           ''')\n",
    "db.commit()\n",
    "print(\"Table created\")\n",
    "\n",
    "c = db.cursor()\n",
    "c.executemany(\"INSERT INTO ga_elect_manual_classification VALUES (?,?, ?, ?)\", input_str_list)\n",
    "db.commit()\n",
    "print(\"Data uploaded\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming that the auxillary table was loaded\n",
    "\n",
    "Confirming that this new table was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auxillary data correctly loaded\n"
     ]
    }
   ],
   "source": [
    "# Test Function/\n",
    "\n",
    "db = sqlite3.connect(full_path_to_db)\n",
    "c = db.cursor()\n",
    "number_of_rows = c.execute('''SELECT count(*) FROM ga_elect_manual_classification''').fetchall()[0][0]\n",
    "db.close()\n",
    "\n",
    "if number_of_rows == 37:\n",
    "    print(\"Auxillary data correctly loaded\")\n",
    "else:\n",
    "    print(\"The auxillary data table appears to be incomplete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
